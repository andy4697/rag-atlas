{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "environment_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.12\n",
      "Environment: /Users/appikatlaanudeep/Documents/projects/rag-atlas/multiagent/bin/python\n",
      "âœ“ Python version compatible\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "python_version = sys.version_info\n",
    "print(f\"Python Version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "if python_version >= (3, 12):\n",
    "    print(\"âœ“ Python version compatible\")\n",
    "else:\n",
    "    print(\"âœ— Need Python 3.12+\")\n",
    "    raise RuntimeError(\"Python version must be 3.12 or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "project_root",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/appikatlaanudeep/Documents/projects/rag-atlas/notebooks\n",
      "âœ“ Project root found: /Users/appikatlaanudeep/Documents/projects/rag-atlas\n"
     ]
    }
   ],
   "source": [
    "# Find Project Root\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# Try different ways to find project root\n",
    "project_root = None\n",
    "\n",
    "# Check if we're in notebooks directory\n",
    "if current_dir.name == \"notebooks\":\n",
    "    project_root = current_dir.parent\n",
    "# Check if we're in project root already\n",
    "elif (current_dir / \"docker-compose.yml\").exists() or (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "# Check if we're one level down\n",
    "elif (current_dir.parent / \"docker-compose.yml\").exists() or (current_dir.parent / \"compose.yml\").exists():\n",
    "    project_root = current_dir.parent\n",
    "\n",
    "if project_root and ((project_root / \"docker-compose.yml\").exists() or (project_root / \"compose.yml\").exists()):\n",
    "    print(f\"âœ“ Project root found: {project_root}\")\n",
    "    # Add project root to Python path\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "        print(\"âœ“ Added project root to Python path\")\n",
    "else:\n",
    "    print(\"âœ— Could not find project root with docker-compose.yml or compose.yml\")\n",
    "    print(\"Available files in current directory:\")\n",
    "    for file in current_dir.iterdir():\n",
    "        print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uv_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ UV: uv 0.9.7 (0adb44480 2025-10-30)\n",
      "âœ“ All required software ready!\n"
     ]
    }
   ],
   "source": [
    "# Check UV Package Manager\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Add UV to PATH\n",
    "os.environ['PATH'] = f\"{os.path.expanduser('~/.local/bin')}:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "# Check UV version\n",
    "try:\n",
    "    result = subprocess.run([\"uv\", \"--version\"], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ“ UV: {result.stdout.strip()}\")\n",
    "        print(\"âœ“ All required software ready!\")\n",
    "    else:\n",
    "        print(\"âœ— UV: Not working\")\n",
    "        \n",
    "except:\n",
    "    print(\"âœ— UV: Not found\")\n",
    "    print(\"Install: curl -LsSf https://astral.sh/uv/install.sh | sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "docker_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Docker is running\n"
     ]
    }
   ],
   "source": [
    "# Check Docker Running\n",
    "try:\n",
    "    result = subprocess.run([\"docker\", \"info\"], capture_output=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ“ Docker is running\")\n",
    "    else:\n",
    "        print(\"âœ— Docker not running - start Docker Desktop\")\n",
    "        exit()\n",
    "except:\n",
    "    print(\"âœ— Docker daemon not accessible\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "service_urls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current containers:\n",
      "  â€¢ airflow: running\n",
      "  â€¢ api: running\n",
      "  â€¢ clickhouse: running\n",
      "  â€¢ opensearch-dashboards: running\n",
      "  â€¢ langfuse: running\n",
      "  â€¢ langfuse-postgres: running\n",
      "  â€¢ ollama: running\n",
      "  â€¢ opensearch: running\n",
      "  â€¢ postgres: running\n",
      "  â€¢ redis: running\n"
     ]
    }
   ],
   "source": [
    "# Check Current Containers\n",
    "import json\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        print(\"Current containers:\")\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    container = json.loads(line)\n",
    "                    service = container.get('Service', 'unknown')\n",
    "                    state = container.get('State', 'unknown')\n",
    "                    print(f\"  â€¢ {service}: {state}\")\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        print(\"No containers running\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Could not check containers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bd07c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE STATUS\n",
      "======================================================================\n",
      "Service              State           Status          Notes\n",
      "----------------------------------------------------------------------\n",
      "âœ“ airflow            running        healthy        Ready\n",
      "âœ“ api                running        healthy        Ready\n",
      "âœ“ clickhouse         running        healthy        Ready\n",
      "âœ“ opensearch-dashboards running        healthy        Ready\n",
      "âš  langfuse           running        unhealthy      Starting up...\n",
      "âœ“ langfuse-postgres  running        healthy        Ready\n",
      "âœ“ ollama             running        healthy        Ready\n",
      "âœ“ opensearch         running        healthy        Ready\n",
      "âœ“ postgres           running        healthy        Ready\n",
      "âœ“ redis              running        healthy        Ready\n"
     ]
    }
   ],
   "source": [
    "# Service Health Check\n",
    "EXPECTED_SERVICES = {\n",
    "    'api': 'FastAPI REST API server',\n",
    "    'postgres': 'PostgreSQL database',\n",
    "    'opensearch': 'OpenSearch search engine', \n",
    "    'opensearch-dashboards': 'OpenSearch web dashboard',\n",
    "    'ollama': 'Local LLM inference server',\n",
    "    'airflow': 'Workflow automation (optional - may be off)'\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=15\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"SERVICE STATUS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Service':<20} {'State':<15} {'Status':<15} {'Notes'}\")\n",
    "        print(\"-\" * 70)\n",
    "    else:\n",
    "        print(\"Could not get service status\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking services: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Parse Service Status\n",
    "found_services = set()\n",
    "service_states = {}\n",
    "\n",
    "if result.stdout.strip():\n",
    "    for line in result.stdout.strip().split('\\n'):\n",
    "        if line.strip():\n",
    "            try:\n",
    "                container = json.loads(line)\n",
    "                service = container.get('Service', 'unknown')\n",
    "                state = container.get('State', 'unknown')\n",
    "                health = container.get('Health', 'no check')\n",
    "                \n",
    "                found_services.add(service)\n",
    "                service_states[service] = {'state': state, 'health': health}\n",
    "\n",
    "                if state == 'running' and health in ['healthy', 'no check']:\n",
    "                    indicator = \"âœ“\"\n",
    "                    notes = \"Ready\"\n",
    "                elif state == 'running' and health == 'unhealthy':\n",
    "                    indicator = \"âš \"\n",
    "                    notes = \"Starting up...\"\n",
    "                elif state == 'exited':\n",
    "                    indicator = \"âœ—\"\n",
    "                    notes = \"Failed to start\"\n",
    "                else:\n",
    "                    indicator = \"?\"\n",
    "                    notes = f\"Status: {state}\"\n",
    "                \n",
    "                print(f\"{indicator} {service:<18} {state:<14} {health:<14} {notes}\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ec1da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TROUBLESHOOTING:\n",
      "   docker compose logs langfuse\n"
     ]
    }
   ],
   "source": [
    "# Check Missing Services\n",
    "missing_services = set(EXPECTED_SERVICES.keys()) - found_services\n",
    "\n",
    "if missing_services:\n",
    "    print(\"\\nMISSING SERVICES:\")\n",
    "    print(\"-\" * 70)\n",
    "    for service in missing_services:\n",
    "        description = EXPECTED_SERVICES[service]\n",
    "        if service == 'airflow':\n",
    "            print(f\"âš  {service:<18} not running    {'(Optional)':<14} {description}\")\n",
    "        else:\n",
    "            print(f\"âœ— {service:<18} not running    {'Required':<14} {description}\")\n",
    "\n",
    "failed_services = [s for s, info in service_states.items() \n",
    "                  if info['state'] in ['exited', 'restarting'] or info['health'] == 'unhealthy']\n",
    "\n",
    "if failed_services:\n",
    "    print(f\"\\nTROUBLESHOOTING:\")\n",
    "    for service in failed_services:\n",
    "        print(f\"   docker compose logs {service}\")\n",
    "elif missing_services and 'airflow' not in missing_services:\n",
    "    print(f\"\\nACTION NEEDED:\")\n",
    "    print(\"Start missing services: docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7711ca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FastAPI is responding\n",
      "Status: healthy\n"
     ]
    }
   ],
   "source": [
    "# Test FastAPI Health\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/api/v1/health/\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"âœ“ FastAPI is responding\")\n",
    "        print(f\"Status: {data.get('status', 'unknown')}\")\n",
    "    else:\n",
    "        print(f\"âš  API returned status: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âœ— API not responding - wait 1-2 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— API test error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853bf8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  PRODUCTION INSIGHT (Online Sessions Only)\n",
      "============================================================\n",
      "â“ How are they scaled?\n",
      "â“ What are the bottlenecks?\n",
      "â“ How are they monitored and managed?\n",
      "â“ How are they integrated with other systems?\n",
      "â“ What are the best practices for using these systems?\n",
      "â“ How are these systems used and deployed in production?\n",
      "â“ How are they tested? in terms of load and performance?\n",
      "â†’ Learn these production secrets in our online walkthrough sessions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# PRODUCTION INSIGHTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  PRODUCTION INSIGHT (Online Sessions Only)\")\n",
    "print(\"=\"*60)\n",
    "print(\"â“ How are they scaled?\")\n",
    "print(\"â“ What are the bottlenecks?\")\n",
    "print(\"â“ How are they monitored and managed?\")\n",
    "print(\"â“ How are they integrated with other systems?\")\n",
    "print(\"â“ What are the best practices for using these systems?\")\n",
    "print(\"â“ How are these systems used and deployed in production?\")\n",
    "print(\"â“ How are they tested? in terms of load and performance?\")\n",
    "print(\"â†’ Learn these production secrets in our online walkthrough sessions!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "403315e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Airflow credentials:\n",
      "  Username: admin\n",
      "  Password: admin\n",
      "  URL: http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "# Get Airflow Password\n",
    "# This setup uses default admin credentials\n",
    "username = \"admin\"\n",
    "password = \"admin\"\n",
    "\n",
    "print(f\"âœ“ Airflow credentials:\")\n",
    "print(f\"  Username: {username}\")\n",
    "print(f\"  Password: {password}\")\n",
    "print(f\"  URL: http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1457cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Airflow is healthy\n",
      "\n",
      "Airflow Login:\n",
      "URL: http://localhost:8080\n",
      "Username: admin\n",
      "Password: admin\n"
     ]
    }
   ],
   "source": [
    "# Test Airflow Health\n",
    "try:\n",
    "    # Try the main Airflow web interface first\n",
    "    response = requests.get(\"http://localhost:8080/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ“ Airflow is healthy\")\n",
    "        print(f\"\\nAirflow Login:\")\n",
    "        print(f\"URL: http://localhost:8080\")\n",
    "        print(f\"Username: admin\")\n",
    "        print(f\"Password: {password}\")\n",
    "    else:\n",
    "        # Try alternative health endpoint\n",
    "        response = requests.get(\"http://localhost:8080\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ“ Airflow web interface is accessible\")\n",
    "            print(f\"\\nAirflow Login:\")\n",
    "            print(f\"URL: http://localhost:8080\")\n",
    "            print(f\"Username: admin\")\n",
    "            print(f\"Password: {password}\")\n",
    "        else:\n",
    "            print(f\"âš  Airflow returned: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âœ— Airflow not responding - wait 2-3 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Airflow test error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eda01d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenSearch Dashboards is accessible!\n",
      "âœ“ Web interface is ready for exploration\n",
      "\n",
      " Web Interface Access:\n",
      "========================================\n",
      "Main Dashboard: http://localhost:5601\n",
      "Dev Tools: http://localhost:5601/app/dev_tools\n",
      "========================================\n",
      "\n",
      " Student Learning Activities:\n",
      "1. Explore the Dashboard:\n",
      "   â€¢ Visit http://localhost:5601\n",
      "   â€¢ Navigate through the interface\n",
      "   â€¢ Check out the 'Discover' tab\n",
      "\n",
      "2. Use Dev Tools for API Queries:\n",
      "   â€¢ Go to Dev Tools\n",
      "   â€¢ Try: GET /_cluster/health\n",
      "   â€¢ Try: GET /_cat/indices?v\n",
      "   â€¢ Try: GET /_cluster/stats\n",
      "   â€¢ Check the learning material for more information\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check OpenSearch Dashboards Web Interface\n",
    "# This is the proper way for students to interact with OpenSearch\n",
    "\n",
    "dashboards_url = \"http://localhost:5601\"\n",
    "\n",
    "try:\n",
    "    # Test if Dashboards is accessible\n",
    "    response = requests.get(f\"{dashboards_url}/api/status\", timeout=10, allow_redirects=True)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ“ OpenSearch Dashboards is accessible!\")\n",
    "        print(\"âœ“ Web interface is ready for exploration\")\n",
    "        \n",
    "        print(\"\\n Web Interface Access:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Main Dashboard: {dashboards_url}\")\n",
    "        print(f\"Dev Tools: {dashboards_url}/app/dev_tools\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(\"\\n Student Learning Activities:\")\n",
    "        print(\"1. Explore the Dashboard:\")\n",
    "        print(\"   â€¢ Visit http://localhost:5601\")\n",
    "        print(\"   â€¢ Navigate through the interface\")\n",
    "        print(\"   â€¢ Check out the 'Discover' tab\")\n",
    "        \n",
    "        print(\"\\n2. Use Dev Tools for API Queries:\")\n",
    "        print(\"   â€¢ Go to Dev Tools\")\n",
    "        print(\"   â€¢ Try: GET /_cluster/health\")\n",
    "        print(\"   â€¢ Try: GET /_cat/indices?v\")\n",
    "        print(\"   â€¢ Try: GET /_cluster/stats\")\n",
    "        print(\"   â€¢ Check the learning material for more information\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âš  Dashboards returned status: {response.status_code}\")\n",
    "        print(\"Interface may still be starting up\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âœ— OpenSearch Dashboards not accessible yet\")\n",
    "    print(\"Wait 2-3 minutes for full startup\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"âš  Dashboards request timed out\")\n",
    "    print(\"This is normal during startup - try again in a few minutes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error accessing Dashboards: {e}\")\n",
    "    print(\"Check container status: docker compose ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "619d5cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ PRODUCTION INSIGHT (Online Sessions Only)\n",
      "============================================================\n",
      "â“ Why companies use OpenSearch?\n",
      "â“ What all is achievable with OpenSearch?\n",
      "â“ How does OpenSearch handle billions of documents?\n",
      "â“ How do companies search through billions of documents?\n",
      "â“ How do e-commerce giants search millions of products instantly?\n",
      "â†’ Learn these production secrets in our online walkthrough sessions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# PRODUCTION DEPLOYMENT INSIGHT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ PRODUCTION INSIGHT (Online Sessions Only)\")\n",
    "print(\"=\"*60)\n",
    "print(\"â“ Why companies use OpenSearch?\")\n",
    "print(\"â“ What all is achievable with OpenSearch?\")\n",
    "print(\"â“ How does OpenSearch handle billions of documents?\")\n",
    "print(\"â“ How do companies search through billions of documents?\")\n",
    "print(\"â“ How do e-commerce giants search millions of products instantly?\")\n",
    "print(\"â†’ Learn these production secrets in our online walkthrough sessions!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf0119ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ollama is running!\n",
      "Available models: 0\n",
      "\n",
      "  No models installed yet\n",
      "   This is normal - models are large files (3-7 GB each)\n",
      "   In Week 4, we'll install a model like llama3.2\n",
      "\n",
      "  Try This Later (Week 4):\n",
      "1. docker exec -it rag-ollama ollama pull llama3.2\n",
      "2. docker exec -it rag-ollama ollama list\n",
      "3. docker exec -it rag-ollama ollama run llama3.2\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check Ollama Service Status\n",
    "# Let's see if Ollama is running and what models are available\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "ollama_url = \"http://localhost:11434/api/tags\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(ollama_url, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models_data = response.json()\n",
    "        models = models_data.get('models', [])\n",
    "        \n",
    "        print(\"âœ“ Ollama is running!\")\n",
    "        print(f\"Available models: {len(models)}\")\n",
    "        \n",
    "        if models:\n",
    "            print(\"\\nInstalled Models:\")\n",
    "            for model in models:\n",
    "                name = model.get('name', 'unknown')\n",
    "                size = model.get('size', 0)\n",
    "                size_gb = round(size / (1024**3), 1)\n",
    "                print(f\"  â€¢ {name} ({size_gb} GB)\")\n",
    "        else:\n",
    "            print(\"\\n  No models installed yet\")\n",
    "            print(\"   This is normal - models are large files (3-7 GB each)\")\n",
    "            print(\"   In Week 4, we'll install a model like llama3.2\")\n",
    "            \n",
    "        print(\"\\n  Try This Later (Week 4):\")\n",
    "        print(\"1. docker exec -it rag-ollama ollama pull llama3.2\")\n",
    "        print(\"2. docker exec -it rag-ollama ollama list\")\n",
    "        print(\"3. docker exec -it rag-ollama ollama run llama3.2\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âš  Ollama returned status: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âœ— Ollama is not responding yet\")\n",
    "    print(\"Ollama service might still be starting\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"âœ— Ollama request timed out\")\n",
    "    print(\"Service might still be initializing\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Unexpected error testing Ollama: {e}\")\n",
    "    print(\"Try again in a few minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "725dd739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ollama API is healthy!\n",
      "Version: 0.11.2\n",
      "\n",
      "  What is Ollama?\n",
      "â€¢ Runs AI models completely on your local machine\n",
      "â€¢ No data sent to external services (privacy-first)\n",
      "â€¢ No API fees or rate limits\n",
      "â€¢ Supports models like Llama, Mistral, Phi, etc.\n",
      "\n",
      "  Coming in Week 4:\n",
      "â€¢ Install and run a local language model\n",
      "â€¢ Generate answers to research questions\n",
      "â€¢ Summarize academic papers\n",
      "â€¢ All processing stays on your computer!\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Check Ollama Version and Health\n",
    "# Let's verify Ollama is properly configured\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "ollama_version_url = \"http://localhost:11434/api/version\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(ollama_version_url, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        version_data = response.json()\n",
    "        version = version_data.get('version', 'unknown')\n",
    "        \n",
    "        print(\"âœ“ Ollama API is healthy!\")\n",
    "        print(f\"Version: {version}\")\n",
    "        \n",
    "        print(\"\\n  What is Ollama?\")\n",
    "        print(\"â€¢ Runs AI models completely on your local machine\")\n",
    "        print(\"â€¢ No data sent to external services (privacy-first)\")\n",
    "        print(\"â€¢ No API fees or rate limits\")\n",
    "        print(\"â€¢ Supports models like Llama, Mistral, Phi, etc.\")\n",
    "        \n",
    "        print(\"\\n  Coming in Week 4:\")\n",
    "        print(\"â€¢ Install and run a local language model\")\n",
    "        print(\"â€¢ Generate answers to research questions\")\n",
    "        print(\"â€¢ Summarize academic papers\")\n",
    "        print(\"â€¢ All processing stays on your computer!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âš  Ollama version check returned: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âœ— Could not check Ollama version\")\n",
    "    print(\"Service might still be starting up\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"âœ— Ollama request timed out\")\n",
    "    print(\"Service might still be initializing\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Unexpected error checking version: {e}\")\n",
    "    print(\"Try again in a few minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "534282b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ PRODUCTION INSIGHT (Online Sessions Only)\n",
      "============================================================\n",
      "â“ What are the real issues with LLMs when in production?\n",
      "â“ What is the difference between fine-tuned LLM and RAG?\n",
      "â“ How do companies serve LLMs without burning through cash?\n",
      "â†’ Learn these production secrets in our online walkthrough sessions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# PRODUCTION DEPLOYMENT INSIGHT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ PRODUCTION INSIGHT (Online Sessions Only)\")\n",
    "print(\"=\"*60)\n",
    "print(\"â“ What are the real issues with LLMs when in production?\")\n",
    "print(\"â“ What is the difference between fine-tuned LLM and RAG?\")\n",
    "print(\"â“ How do companies serve LLMs without burning through cash?\")\n",
    "print(\"â†’ Learn these production secrets in our online walkthrough sessions!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ccc1955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADING LLAMA 3.2:1B MODEL\n",
      "==================================================\n",
      "This is a small 1.3GB model - perfect for testing!\n",
      "Download will take 2-5 minutes depending on your internet speed...\n",
      "Llama 3.2:1b model downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# HANDS-ON: Pull and Test Llama 3.2 (Small Model)\n",
    "\n",
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"DOWNLOADING LLAMA 3.2:1B MODEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This is a small 1.3GB model - perfect for testing!\")\n",
    "print(\"Download will take 2-5 minutes depending on your internet speed...\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"exec\", \"rag-ollama\", \"ollama\", \"pull\", \"llama3.2:1b\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=600\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Llama 3.2:1b model downloaded successfully!\")\n",
    "    else:\n",
    "        print(f\"Download issue: {result.stderr}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"Download timed out - this is normal for slow connections\")\n",
    "    print(\"The download continues in the background\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading model: {e}\")\n",
    "    print(\"Make sure Ollama container is running: docker compose ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7691680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing llama3.2:1b with prompt: 'What is machine learning in one sentence?'\n",
      "------------------------------------------------------------\n",
      "Generating response (this may take 10-30 seconds)...\n",
      "Response generated in 25.8 seconds\n",
      "\n",
      "RESPONSE:\n",
      "========================================\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data, make predictions or decisions without being explicitly programmed, and improve their performance over time through experience and iteration.\n",
      "========================================\n",
      "\n",
      "Model: llama3.2:1b\n",
      "Generation time: 25755ms\n",
      "\n",
      "SUCCESS! Your local AI model is working!\n",
      "\n",
      "Try more prompts:\n",
      "â€¢ test_ollama_model(\"llama3.2:1b\", \"Explain neural networks simply\")\n",
      "â€¢ test_ollama_model(\"llama3.2:1b\", \"Write a Python function to sort a list\")\n"
     ]
    }
   ],
   "source": [
    "# Test Llama 3.2:1b API\n",
    "\n",
    "def test_ollama_model(model_name, prompt, max_wait_time=180):\n",
    "    \"\"\"Test an Ollama model with a prompt.\"\"\"\n",
    "    print(f\"Testing {model_name} with prompt: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"Generating response (this may take 10-30 seconds)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = requests.post(url, json=data, timeout=max_wait_time)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            response_text = result.get('response', '').strip()\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Response generated in {elapsed_time:.1f} seconds\")\n",
    "            print(\"\\nRESPONSE:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(response_text)\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "            if 'model' in result:\n",
    "                print(f\"\\nModel: {result['model']}\")\n",
    "            if 'total_duration' in result:\n",
    "                duration_ms = result['total_duration'] / 1000000\n",
    "                print(f\"Generation time: {duration_ms:.0f}ms\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"API error: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Could not connect to Ollama API\")\n",
    "        print(\"Make sure Ollama is running: docker compose ps\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Request timed out\")\n",
    "        print(\"Model might be loading for the first time (this is normal)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "test_prompt = \"What is machine learning in one sentence?\"\n",
    "success = test_ollama_model(\"llama3.2:1b\", test_prompt)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nSUCCESS! Your local AI model is working!\")\n",
    "    print(\"\\nTry more prompts:\")\n",
    "    print('â€¢ test_ollama_model(\"llama3.2:1b\", \"Explain neural networks simply\")')\n",
    "    print('â€¢ test_ollama_model(\"llama3.2:1b\", \"Write a Python function to sort a list\")')\n",
    "else:\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure model downloaded: docker exec rag-ollama ollama list\")\n",
    "    print(\"2. Check Ollama logs: docker compose logs ollama\")\n",
    "    print(\"3. Try again - first run takes longer to load model into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "test_api",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PostgreSQL is accepting connections on port 5432!\n",
      "\n",
      "  Database Connection Details:\n",
      "â€¢ Host: localhost\n",
      "â€¢ Port: 5432\n",
      "â€¢ Database: rag_db\n",
      "â€¢ Username: rag_user\n",
      "â€¢ Password: rag_password\n",
      "\n",
      "  Recommended GUI Tools:\n",
      "â€¢ DBeaver (Free): https://dbeaver.io/download/\n",
      "â€¢ pgAdmin: https://www.pgadmin.org/download/\n"
     ]
    }
   ],
   "source": [
    " #Test 1: Check PostgreSQL Connection (Basic)\n",
    "# Let's verify PostgreSQL is accepting connections\n",
    "\n",
    "def test_postgres_connection():\n",
    "    \"\"\"Test PostgreSQL connection using simple socket check.\"\"\"\n",
    "    import socket\n",
    "    \n",
    "    try:\n",
    "        # Test if PostgreSQL port is open\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(3)\n",
    "        result = sock.connect_ex(('localhost', 5432))\n",
    "        sock.close()\n",
    "        \n",
    "        if result == 0:\n",
    "            print(\"âœ“ PostgreSQL is accepting connections on port 5432!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âœ— PostgreSQL port is not accessible\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Could not test PostgreSQL: {e}\")\n",
    "        return False\n",
    "\n",
    "postgres_available = test_postgres_connection()\n",
    "\n",
    "if postgres_available:\n",
    "    print(\"\\n  Database Connection Details:\")\n",
    "    print(\"â€¢ Host: localhost\")\n",
    "    print(\"â€¢ Port: 5432\") \n",
    "    print(\"â€¢ Database: rag_db\")\n",
    "    print(\"â€¢ Username: rag_user\")\n",
    "    print(\"â€¢ Password: rag_password\")\n",
    "    \n",
    "    print(\"\\n  Recommended GUI Tools:\")\n",
    "    print(\"â€¢ DBeaver (Free): https://dbeaver.io/download/\")\n",
    "    print(\"â€¢ pgAdmin: https://www.pgadmin.org/download/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62aef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PostgreSQL connected\n"
     ]
    }
   ],
   "source": [
    "# Test PostgreSQL Connection\n",
    "try:\n",
    "    import psycopg2\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=5432,\n",
    "        database=\"rag_db\", \n",
    "        user=\"rag_user\",\n",
    "        password=\"rag_password\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ PostgreSQL connected\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš  psycopg2 not installed - basic connection only\")\n",
    "    # exit()\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Database connection failed: {e}\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efb919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 total tables\n",
      "Application tables: 0\n",
      "Airflow tables: 0\n",
      "  No application tables yet (expected in Week 1)\n"
     ]
    }
   ],
   "source": [
    "# Check Database Tables\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'public'\n",
    "    ORDER BY table_name;\n",
    "\"\"\")\n",
    "\n",
    "all_tables = cursor.fetchall()\n",
    "\n",
    "app_tables = []\n",
    "airflow_tables = []\n",
    "\n",
    "for (table_name,) in all_tables:\n",
    "    if table_name in ['papers', 'users', 'embeddings']:\n",
    "        app_tables.append(table_name)\n",
    "    else:\n",
    "        airflow_tables.append(table_name)\n",
    "\n",
    "print(f\"Found {len(all_tables)} total tables\")\n",
    "print(f\"Application tables: {len(app_tables)}\")\n",
    "print(f\"Airflow tables: {len(airflow_tables)}\")\n",
    "\n",
    "for table in app_tables:\n",
    "    print(f\"  â€¢ {table}\")\n",
    "\n",
    "if not app_tables:\n",
    "    print(\"  No application tables yet\")\n",
    "    \n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d703df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ PRODUCTION INSIGHT (Online Sessions Only)\n",
      "============================================================\n",
      "â“ How do companies handle millions of transactions with PostgreSQL?\n",
      "â“ What's the secret to zero-downtime database migrations?\n",
      "â†’ Learn these production secrets in our online walkthrough sessions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# PRODUCTION DEPLOYMENT INSIGHT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ PRODUCTION INSIGHT (Online Sessions Only)\")\n",
    "print(\"=\"*60)\n",
    "print(\"â“ How do companies handle millions of transactions with PostgreSQL?\")\n",
    "print(\"â“ What's the secret to zero-downtime database migrations?\")\n",
    "print(\"â†’ Learn these production secrets in our online walkthrough sessions!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
